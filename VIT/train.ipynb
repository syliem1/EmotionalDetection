{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f5bd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (0.20.1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (from torch) (4.15.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (from torchvision) (2.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.11.3-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nathan\\miniconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->transformers) (2025.10.5)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.0/6.2 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 2.9/6.2 MB 7.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.5/6.2 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 9.5 MB/s  0:00:00\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 15.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.9/12.0 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.0 MB 18.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 18.3 MB/s  0:00:00\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 566.1/566.1 kB 19.0 MB/s  0:00:00\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 19.3 MB/s  0:00:00\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Downloading regex-2025.11.3-cp311-cp311-win_amd64.whl (277 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, sympy, safetensors, regex, fsspec, huggingface-hub, tokenizers, transformers\n",
      "\n",
      "  Attempting uninstall: sympy\n",
      "\n",
      "    Found existing installation: sympy 1.14.0\n",
      "\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "    Uninstalling sympy-1.14.0:\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "      Successfully uninstalled sympy-1.14.0\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   --------------- ------------------------ 3/8 [regex]\n",
      "   -------------------- ------------------- 4/8 [fsspec]\n",
      "   ------------------------- -------------- 5/8 [huggingface-hub]\n",
      "   ------------------------- -------------- 5/8 [huggingface-hub]\n",
      "   ------------------------- -------------- 5/8 [huggingface-hub]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ----------------------------------- ---- 7/8 [transformers]\n",
      "   ---------------------------------------- 8/8 [transformers]\n",
      "\n",
      "Successfully installed fsspec-2025.10.0 huggingface-hub-0.36.0 regex-2025.11.3 safetensors-0.7.0 sympy-1.13.1 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision transformers scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f3c5659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import ViTConfig, ViTForImageClassification\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d81ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_directory, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.img_directory = img_directory\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_col = 'image'\n",
    "        self.label_col = 'emotion'\n",
    "        self.data_frame[self.label_col] = self.data_frame[self.label_col].astype(str).str.lower() # normalize labels to lowercase\n",
    "\n",
    "        # create label mappings\n",
    "        unique_labels = sorted(self.data_frame[self.label_col].unique())\n",
    "        self.label_to_int = {label: i for i, label in enumerate(unique_labels)} # {'angry': 0, 'happy': 1, ...}\n",
    "                                                                                # convert string labels to integers for pytorch to use as classification targets\n",
    "        self.int_to_label = {i: label for i, label in enumerate(unique_labels)} # {0: 'angry', 1: 'happy', ...}\n",
    "                                                                                # convert integers back to string labels for easier interpretation of results\n",
    "\n",
    "        print(f\"Loaded {len(self.data_frame)} samples from {img_directory}. Classes: {self.label_to_int}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''Get image and label at index idx'''\n",
    "\n",
    "        image_name = self.data_frame.iloc[idx][self.image_col]\n",
    "        image_path = os.path.join(self.img_directory, image_name)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        label_str = self.data_frame.iloc[idx][self.label_col].lower()\n",
    "        label_idx = self.label_to_int[label_str]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a25f6a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration for model\n",
    "BATCH_SIZE = 64\n",
    "NUM_CLASSES = 8\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db58b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13690 samples from data/train. Classes: {'anger': 0, 'contempt': 1, 'disgust': 2, 'fear': 3, 'happiness': 4, 'neutral': 5, 'sadness': 6, 'surprise': 7}\n"
     ]
    }
   ],
   "source": [
    "# load datasets\n",
    "train_dataset = EmotionDataset(\n",
    "    csv_file='../data/legend.csv',\n",
    "    img_directory='../images',\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dc86900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): ConvolutionStem(\n",
       "          (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-7): 8 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=256, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model setup\n",
    "config = ViTConfig(\n",
    "    image_size = 224,\n",
    "    patch_size = 16,\n",
    "    num_labels = NUM_CLASSES,\n",
    "    hidden_size = 256,\n",
    "    num_hidden_layers = 8,\n",
    "    num_attention_heads = 4,\n",
    "    intermediate_size = 512,\n",
    "    hidden_dropout_prob = 0.1\n",
    ")\n",
    "\n",
    "model = ViTForImageClassification(config)\n",
    "\n",
    "class ConvolutionStem(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,64,kernel_size=3,stride=2,padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64,128,kernel_size=3,stride=2,padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128,256,kernel_size=3,stride=2,padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256,hidden_size,kernel_size=3,stride=2,padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        x = self.relu(self.bn1(self.conv1(pixel_values)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        return x.flatten(2).transpose(1,2)\n",
    "    \n",
    "model.vit.embeddings.patch_embeddings.projection = ConvolutionStem(hidden_size=config.hidden_size)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ec31a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train\\\\KatrinaKaif_35.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m model.train()\n\u001b[32m      9\u001b[39m running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nathan\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nathan\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nathan\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nathan\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mEmotionDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     26\u001b[39m image_name = \u001b[38;5;28mself\u001b[39m.data_frame.iloc[idx][\u001b[38;5;28mself\u001b[39m.image_col]\n\u001b[32m     27\u001b[39m image_path = os.path.join(\u001b[38;5;28mself\u001b[39m.img_directory, image_name)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m image = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m.convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     30\u001b[39m label_str = \u001b[38;5;28mself\u001b[39m.data_frame.iloc[idx][\u001b[38;5;28mself\u001b[39m.label_col].lower()\n\u001b[32m     31\u001b[39m label_idx = \u001b[38;5;28mself\u001b[39m.label_to_int[label_str]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nathan\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\PIL\\Image.py:3465\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3462\u001b[39m     filename = os.fspath(fp)\n\u001b[32m   3464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[32m-> \u001b[39m\u001b[32m3465\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3466\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3467\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/train\\\\KatrinaKaif_35.jpg'"
     ]
    }
   ],
   "source": [
    "# execute\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 10\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
